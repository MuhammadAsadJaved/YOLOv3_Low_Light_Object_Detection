{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import core.utils as utils\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from core.yolov3 import YOLOv3, decode, compute_loss\n",
    "from PIL import Image\n",
    "from core.config import cfg\n",
    "from core.dataset import Dataset\n",
    "import os\n",
    "import time\n",
    "assert(tf.__version__=='2.0.0-beta1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training YoloV3 on dark data\"\"\"\n",
    "\n",
    "\"\"\"change __C.YOLO.CLASSES= \"./data/classes/coco.names\" to \"./data/classes/darkdata.names\" in core/config.py\"\"\"\n",
    "\n",
    "\n",
    "trainset = Dataset('train')\n",
    "logdir = \"./data/logdemo\"\n",
    "steps_per_epoch = len(trainset)\n",
    "global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n",
    "warmup_steps = cfg.TRAIN.WARMUP_EPOCHS * steps_per_epoch\n",
    "total_steps = cfg.TRAIN.EPOCHS * steps_per_epoch\n",
    "\n",
    "input_tensor = tf.keras.layers.Input([416, 416,3])\n",
    "conv_tensors = YOLOv3(input_tensor)\n",
    "\n",
    "output_tensors = []\n",
    "for i, conv_tensor in enumerate(conv_tensors):\n",
    "    pred_tensor = decode(conv_tensor, i)\n",
    "    output_tensors.append(conv_tensor)\n",
    "    output_tensors.append(pred_tensor)\n",
    "\n",
    "model = tf.keras.Model(input_tensor, output_tensors)\n",
    "\n",
    "'''load weights of subset of classes from coco trained yolov3 weights'''\n",
    "\n",
    "num = [0,1,2]\n",
    "weight_num = []\n",
    "for i in num:\n",
    "    tmp=[(85*i),(85*i+1),(85*i+2),(85*i+3),(85*i+4),(85*i+4+2),(85*i+4+9),(85*i+4+40),(85*i+4+6),(85*i+4+3),(85*i+4+16),(85*i+4+57),(85*i+4+42),(85*i+4+17),(85*i+4+4),(85*i+4+1),(85*i+4+61)]\n",
    "    weight_num.append(tmp)\n",
    "req_act = []\n",
    "for each in weight_num:\n",
    "    for ea in each:\n",
    "        req_act.append(ea)\n",
    "        \n",
    "wf = open('./yolov3.weights', 'rb')\n",
    "major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
    "j = 0\n",
    "layer_bias = []\n",
    "layer_weights = []\n",
    "for i in range(75):\n",
    "    conv_layer_name = 'conv2d_%d' %i if i > 0 else 'conv2d'\n",
    "    bn_layer_name = 'batch_normalization_%d' %j if j > 0 else 'batch_normalization'\n",
    "\n",
    "    conv_layer = model.get_layer(conv_layer_name)\n",
    "    filters = conv_layer.filters\n",
    "    k_size = conv_layer.kernel_size[0]\n",
    "    in_dim = conv_layer.input_shape[-1]\n",
    "    if i not in [58, 66, 74]:\n",
    "            # darknet weights: [beta, gamma, mean, variance]\n",
    "        bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n",
    "            # tf weights: [gamma, beta, mean, variance]\n",
    "        bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
    "        bn_layer = model.get_layer(bn_layer_name)\n",
    "        j += 1\n",
    "        \n",
    "    else:\n",
    "        conv_bias = np.fromfile(wf, dtype=np.float32, count=255)\n",
    "        layer_bias.append(conv_bias)\n",
    "    if i not in [58, 66, 74]:        \n",
    "        conv_shape = (filters, in_dim, k_size, k_size)\n",
    "        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
    "        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
    "        conv_layer.set_weights([conv_weights])\n",
    "        bn_layer.set_weights(bn_weights)\n",
    "    elif i==58:\n",
    "        conv_shape = (255,1024,1,1)\n",
    "        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
    "        # tf shape (height, width, in_dim, out_dim)\n",
    "        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
    "        layer_weights.append(conv_weights)\n",
    "        conv_layer.set_weights([conv_weights[:,:,:,req_act], conv_bias[req_act]])\n",
    "    elif i==66:\n",
    "        conv_shape = (255,512,1,1)\n",
    "        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
    "        # tf shape (height, width, in_dim, out_dim)\n",
    "        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
    "        layer_weights.append(conv_weights)\n",
    "        conv_layer.set_weights([conv_weights[:,:,:,req_act], conv_bias[req_act]])\n",
    "    else:\n",
    "        conv_shape = (255,256,1,1)\n",
    "        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
    "        # tf shape (height, width, in_dim, out_dim)\n",
    "        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
    "        layer_weights.append(conv_weights)\n",
    "        conv_layer.set_weights([conv_weights[:,:,:,req_act], conv_bias[req_act]])\n",
    "#print(len(wf.read()))\n",
    "assert len(wf.read()) == 0, 'failed to read all data'\n",
    "wf.close()\n",
    "\n",
    "''' freezing all layers except output layers to fine tune on dark dataset'''\n",
    "for layer in model.layers:\n",
    "    if layer.name in model.output_names:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "'''train the model from here'''\n",
    "model.summary()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "if os.path.exists(logdir): shutil.rmtree(logdir)\n",
    "writer = tf.summary.create_file_writer(logdir) ##\n",
    "\n",
    "def train_step(image_data, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_result = model(image_data, training=True)\n",
    "        giou_loss=conf_loss=prob_loss=0\n",
    "\n",
    "        # optimizing process\n",
    "        for i in range(3):\n",
    "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
    "            loss_items = compute_loss(pred, conv, *target[i], i)\n",
    "            giou_loss += loss_items[0]\n",
    "            conf_loss += loss_items[1]\n",
    "            prob_loss += loss_items[2]\n",
    "\n",
    "        total_loss = giou_loss + conf_loss + prob_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        tf.print(\"=> STEP %4d   lr: %.6f   giou_loss: %4.2f   conf_loss: %4.2f   \"\n",
    "                 \"prob_loss: %4.2f   total_loss: %4.2f\" %(global_steps, optimizer.lr.numpy(),\n",
    "                                                          giou_loss, conf_loss,\n",
    "                                                          prob_loss, total_loss))\n",
    "        # update learning rate\n",
    "        global_steps.assign_add(1)\n",
    "        if global_steps < warmup_steps:\n",
    "            lr = global_steps / warmup_steps *cfg.TRAIN.LR_INIT\n",
    "        else:\n",
    "            lr = cfg.TRAIN.LR_END + 0.5 * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (\n",
    "                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))\n",
    "            )\n",
    "        optimizer.lr.assign(lr.numpy())\n",
    "\n",
    "        # writing summary data to logs to display progress in tensor board\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar(\"lr\", optimizer.lr, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/total_loss\", total_loss, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/giou_loss\", giou_loss, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/conf_loss\", conf_loss, step=global_steps)\n",
    "            tf.summary.scalar(\"loss/prob_loss\", prob_loss, step=global_steps)\n",
    "        writer.flush()\n",
    "\n",
    "for epoch in range(cfg.TRAIN.EPOCHS):\n",
    "    print(\"epoch %4d/%4d begins here:\"%(epoch+1,cfg.TRAIN.EPOCHS),\" steps per epoch: \",steps_per_epoch)\n",
    "    for image_data, target,unhandled in trainset:\n",
    "        #print(target)\n",
    "        train_step(image_data, target)\n",
    "    model.save_weights(\"./yolov3_dark\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
